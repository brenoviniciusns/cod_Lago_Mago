{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d41f8e47-1690-4671-b350-8e38dacc87a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType\n",
    "def ler_dados(Ano): \n",
    "    schema = StructType([\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"owner_id\", StringType(), True),\n",
    "        StructField(\"parent_id\", StringType(), nullable=True),\n",
    "        StructField(\"slug\", StringType(), True),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"status\", StringType(), True),\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"source_url\", StringType(), True),\n",
    "        StructField(\"created_at\", StringType(), True),\n",
    "        StructField(\"updated_at\", StringType(), True),\n",
    "        StructField(\"published_at\", StringType(), True),\n",
    "        StructField(\"deleted_at\", StringType(), nullable=True),\n",
    "        StructField(\"owner_username\", StringType(), True),\n",
    "        StructField(\"tabcoins\", IntegerType(), True),\n",
    "        StructField(\"tabcoins_credit\", IntegerType(), True),\n",
    "        StructField(\"tabcoins_debit\", IntegerType(), True),\n",
    "        StructField(\"children_deep_count\", IntegerType(), True)\n",
    "    ])\n",
    "    base_path = f\"/Volumes/ingestao_dados/tabnews/dadostabnews/Ano_{Ano}\"\n",
    "    df_ano = spark.read.format(\"json\").option(\"multiLine\", True).schema(schema).load(base_path + \"/**/**/**/**/*.json\")\n",
    "    return df_ano\n",
    "\n",
    "def gravar_dados(df_ano, ano):\n",
    "    df = df_ano\n",
    "    df.write   .format(\"parquet\")\\\n",
    "                    .mode(\"overwrite\")\\\n",
    "                    .option(\"compression\", \"snappy\") \\\n",
    "                    .save(f\"/Volumes/ingestao_dados/tabnews/dadostabnews/Parquet/{ano}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c12e8ff-1ac0-42af-8db6-9c5f12eb435c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for ano in range(2022, 2025):\n",
    "        df_ano = ler_dados(ano)\n",
    "        gravar_dados(df_ano, ano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54ca1f67-e264-42cd-a72f-a23689e0cdb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ano = ler_dados(2025)\n",
    "gravar_dados(df_ano, 2025)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Fazendo o arquivo Parquet",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
