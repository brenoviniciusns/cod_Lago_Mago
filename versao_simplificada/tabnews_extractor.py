#!/usr/bin/env python3
"""
TabNews Data Engineering Scraper
=====================================

Este script coleta publica√ß√µes do TabNews relacionadas a Engenharia de Dados
e salva os resultados em um arquivo Excel.

Funcionalidades:
- Busca publica√ß√µes da API do TabNews
- Filtra por palavras-chave relacionadas a engenharia de dados
- Busca o conte√∫do completo dos posts relevantes
- Salva tudo em um arquivo Excel (.xlsx)

Autor: Vers√£o Simplificada do Projeto Lago Mago
"""

import requests
import pandas as pd
import asyncio
import aiohttp
import json
import logging
import ssl
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path

# Desabilitar verifica√ß√£o SSL para ambientes corporativos
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TabNewsDataExtractor:
    """Classe principal para extrair dados do TabNews relacionados √† Engenharia de Dados"""
    
    def __init__(self):
        self.base_url = "https://www.tabnews.com.br/api/v1/contents"
        self.session = None
        
        # Palavras-chave relacionadas √† Engenharia de Dados
        self.palavras_chave = [
            'ETL',
            'Data Engineering',
            'Engenharia de Dados',
            'Data Pipeline',
            'Apache Spark',
            'Airflow',
            'Kafka',
            'Data Lake',
            'Data Warehouse',
            'Big Data',
            'Databricks',
            'Snowflake',
            'dbt',
            'Python Data',
            'SQL',
            'NoSQL',
            'MongoDB',
            'PostgreSQL',
            'Redis',
            'Elasticsearch',
            'Docker',
            'Kubernetes',
            'AWS Data',
            'Azure Data',
            'GCP Data',
            'Hadoop',
            'Hive',
            'Presto',
            'Trino',
            'Delta Lake',
            'Parquet',
            'Apache Arrow',
            'Stream Processing',
            'Real-time Data',
            'Batch Processing',
            'Data Quality',
            'Data Governance',
            'Data Modeling',
            'Data Transformation',
            'API REST',
            'GraphQL',
            'Microservices',
            'Event Driven',
            'CQRS',
            'Event Sourcing'
        ]
    
    def buscar_publicacoes(self, paginas: int = 5, por_pagina: int = 30) -> List[Dict]:
        """
        Busca publica√ß√µes do TabNews
        
        Args:
            paginas: N√∫mero de p√°ginas para buscar
            por_pagina: N√∫mero de itens por p√°gina
            
        Returns:
            Lista de publica√ß√µes
        """
        logger.info(f"Iniciando busca de publica√ß√µes - {paginas} p√°ginas, {por_pagina} por p√°gina")
        
        todas_publicacoes = []
        
        for pagina in range(1, paginas + 1):
            try:
                params = {
                    'page': pagina,
                    'per_page': por_pagina,
                    'strategy': 'new'  # Estrat√©gia para buscar novos conte√∫dos
                }
                
                logger.info(f"Buscando p√°gina {pagina}...")
                response = requests.get(self.base_url, params=params, timeout=10, verify=False)
                response.raise_for_status()
                
                publicacoes = response.json()
                todas_publicacoes.extend(publicacoes)
                
                logger.info(f"P√°gina {pagina}: {len(publicacoes)} publica√ß√µes obtidas")
                
            except requests.exceptions.RequestException as e:
                logger.error(f"Erro ao buscar p√°gina {pagina}: {e}")
                continue
                
            except json.JSONDecodeError as e:
                logger.error(f"Erro ao decodificar JSON da p√°gina {pagina}: {e}")
                continue
        
        logger.info(f"Total de publica√ß√µes coletadas: {len(todas_publicacoes)}")
        return todas_publicacoes
    
    def filtrar_por_palavras_chave(self, publicacoes: List[Dict]) -> List[Dict]:
        """
        Filtra publica√ß√µes por palavras-chave relacionadas √† engenharia de dados
        
        Args:
            publicacoes: Lista de publica√ß√µes
            
        Returns:
            Lista de publica√ß√µes filtradas
        """
        logger.info("Iniciando filtragem por palavras-chave...")
        
        publicacoes_relevantes = []
        
        for pub in publicacoes:
            titulo = pub.get('title', '').lower()
            
            # Verifica se alguma palavra-chave est√° no t√≠tulo
            palavras_encontradas = []
            for palavra in self.palavras_chave:
                if palavra.lower() in titulo:
                    palavras_encontradas.append(palavra)
            
            if palavras_encontradas:
                pub['palavras_chave_encontradas'] = ', '.join(palavras_encontradas)
                publicacoes_relevantes.append(pub)
        
        logger.info(f"Publica√ß√µes relevantes encontradas: {len(publicacoes_relevantes)}")
        return publicacoes_relevantes
    
    async def buscar_conteudo_completo_async(self, session: aiohttp.ClientSession, 
                                           autor: str, slug: str) -> Tuple[Optional[Dict], str]:
        """
        Busca o conte√∫do completo de uma publica√ß√£o de forma ass√≠ncrona
        
        Args:
            session: Sess√£o aiohttp
            autor: Nome do autor
            slug: Slug da publica√ß√£o
            
        Returns:
            Tupla com (dados, url)
        """
        url = f"{self.base_url}/{autor}/{slug}"
        
        try:
            # Criar contexto SSL que ignora verifica√ß√£o de certificado
            ssl_context = ssl.create_default_context()
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE
            
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=10), ssl=ssl_context) as response:
                if response.status == 200:
                    dados = await response.json()
                    return dados, url
                else:
                    logger.warning(f"Status {response.status} para {url}")
                    return None, url
                    
        except asyncio.TimeoutError:
            logger.error(f"Timeout para {url}")
            return None, url
        except Exception as e:
            logger.error(f"Erro ao buscar {url}: {e}")
            return None, url
    
    async def buscar_todos_conteudos_completos(self, publicacoes_relevantes: List[Dict]) -> List[Dict]:
        """
        Busca o conte√∫do completo de todas as publica√ß√µes relevantes
        
        Args:
            publicacoes_relevantes: Lista de publica√ß√µes filtradas
            
        Returns:
            Lista de publica√ß√µes com conte√∫do completo
        """
        logger.info("Iniciando busca de conte√∫dos completos...")
        
        publicacoes_completas = []
        
        # Limitar requisi√ß√µes simult√¢neas para n√£o sobrecarregar a API
        semaforo = asyncio.Semaphore(5)
        
        async def buscar_com_limite(pub):
            async with semaforo:
                autor = pub.get('owner_username')
                slug = pub.get('slug')
                
                if not autor or not slug:
                    return pub
                
                ssl_context = ssl.create_default_context()
                ssl_context.check_hostname = False
                ssl_context.verify_mode = ssl.CERT_NONE
                connector = aiohttp.TCPConnector(ssl=ssl_context)
                
                async with aiohttp.ClientSession(connector=connector) as session:
                    dados_completos, url = await self.buscar_conteudo_completo_async(session, autor, slug)
                    
                    if dados_completos:
                        # Mescla os dados b√°sicos com o conte√∫do completo
                        pub_completa = {**pub, **dados_completos}
                        pub_completa['api_url'] = url
                        return pub_completa
                    else:
                        pub['api_url'] = url
                        pub['erro_conteudo'] = True
                        return pub
        
        # Executa todas as requisi√ß√µes de forma ass√≠ncrona
        tasks = [buscar_com_limite(pub) for pub in publicacoes_relevantes]
        publicacoes_completas = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filtra exce√ß√µes
        publicacoes_validas = []
        for pub in publicacoes_completas:
            if isinstance(pub, Exception):
                logger.error(f"Exce√ß√£o no processamento: {pub}")
            else:
                publicacoes_validas.append(pub)
        
        logger.info(f"Conte√∫dos completos obtidos: {len(publicacoes_validas)}")
        return publicacoes_validas
    
    def preparar_dados_para_excel(self, publicacoes: List[Dict]) -> pd.DataFrame:
        """
        Prepara os dados para serem salvos no Excel
        
        Args:
            publicacoes: Lista de publica√ß√µes completas
            
        Returns:
            DataFrame pandas
        """
        logger.info("Preparando dados para Excel...")
        
        dados_excel = []
        
        for pub in publicacoes:
            dados_excel.append({
                'ID': pub.get('id', ''),
                'T√≠tulo': pub.get('title', ''),
                'Autor': pub.get('owner_username', ''),
                'Data Cria√ß√£o': pub.get('created_at', ''),
                'Data Atualiza√ß√£o': pub.get('updated_at', ''),
                'URL Fonte': pub.get('source_url', ''),
                'TabCoins': pub.get('tabcoins', 0),
                'Palavras-chave Encontradas': pub.get('palavras_chave_encontradas', ''),
                'URL da API': pub.get('api_url', ''),
                'Slug': pub.get('slug', ''),
                'Status': pub.get('status', ''),
                'Tipo': pub.get('type', ''),
                'Tem Erro': pub.get('erro_conteudo', False),
                'Conte√∫do (In√≠cio)': (pub.get('body', '') or '')[:500] + '...' if pub.get('body') else 'Sem conte√∫do',
                'Contagem de Filhos': pub.get('children_deep_count', 0)
            })
        
        df = pd.DataFrame(dados_excel)
        
        # Converte datas para formato mais leg√≠vel
        for col in ['Data Cria√ß√£o', 'Data Atualiza√ß√£o']:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce').dt.strftime('%d/%m/%Y %H:%M')
        
        # Ordena por TabCoins (mais relevantes primeiro)
        df = df.sort_values('TabCoins', ascending=False).reset_index(drop=True)
        
        logger.info(f"DataFrame preparado com {len(df)} registros")
        return df
    
    def salvar_excel(self, df: pd.DataFrame, caminho_arquivo: str = None) -> str:
        """
        Salva o DataFrame em arquivo Excel
        
        Args:
            df: DataFrame a ser salvo
            caminho_arquivo: Caminho do arquivo (opcional)
            
        Returns:
            Caminho do arquivo salvo
        """
        if caminho_arquivo is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            caminho_arquivo = f"publicacoes_engenharia_dados_{timestamp}.xlsx"
        
        caminho_completo = Path(caminho_arquivo).resolve()
        
        logger.info(f"Salvando arquivo Excel: {caminho_completo}")
        
        try:
            with pd.ExcelWriter(caminho_completo, engine='openpyxl') as writer:
                # Planilha principal com os dados
                df.to_excel(writer, sheet_name='Publica√ß√µes', index=False)
                
                # Planilha com estat√≠sticas
                stats_data = {
                    'M√©trica': [
                        'Total de Publica√ß√µes',
                        'Publica√ß√µes com Conte√∫do',
                        'Publica√ß√µes com Erro',
                        'M√©dia de TabCoins',
                        'Total de TabCoins',
                        'Autores √önicos',
                        'Data da Extra√ß√£o'
                    ],
                    'Valor': [
                        len(df),
                        len(df[df['Tem Erro'] == False]),
                        len(df[df['Tem Erro'] == True]),
                        df['TabCoins'].mean(),
                        df['TabCoins'].sum(),
                        df['Autor'].nunique(),
                        datetime.now().strftime('%d/%m/%Y %H:%M:%S')
                    ]
                }
                
                stats_df = pd.DataFrame(stats_data)
                stats_df.to_excel(writer, sheet_name='Estat√≠sticas', index=False)
                
                # Planilha com as palavras-chave usadas
                palavras_df = pd.DataFrame({
                    'Palavras-chave Utilizadas': self.palavras_chave
                })
                palavras_df.to_excel(writer, sheet_name='Palavras-chave', index=False)
            
            logger.info(f"Arquivo Excel salvo com sucesso: {caminho_completo}")
            return str(caminho_completo)
            
        except Exception as e:
            logger.error(f"Erro ao salvar arquivo Excel: {e}")
            raise
    
    async def executar_pipeline_completo(self, paginas: int = 5, por_pagina: int = 30, 
                                       arquivo_saida: str = None) -> str:
        """
        Executa o pipeline completo de extra√ß√£o de dados
        
        Args:
            paginas: N√∫mero de p√°ginas para buscar
            por_pagina: Itens por p√°gina
            arquivo_saida: Nome do arquivo de sa√≠da
            
        Returns:
            Caminho do arquivo Excel gerado
        """
        logger.info("=== INICIANDO PIPELINE DE EXTRA√á√ÉO TABNEWS ===")
        
        try:
            # 1. Buscar publica√ß√µes
            publicacoes = self.buscar_publicacoes(paginas, por_pagina)
            
            if not publicacoes:
                raise ValueError("Nenhuma publica√ß√£o foi encontrada")
            
            # 2. Filtrar por palavras-chave
            publicacoes_relevantes = self.filtrar_por_palavras_chave(publicacoes)
            
            if not publicacoes_relevantes:
                logger.warning("Nenhuma publica√ß√£o relevante encontrada com as palavras-chave")
                return None
            
            # 3. Buscar conte√∫do completo
            publicacoes_completas = await self.buscar_todos_conteudos_completos(publicacoes_relevantes)
            
            # 4. Preparar dados para Excel
            df = self.preparar_dados_para_excel(publicacoes_completas)
            
            # 5. Salvar em Excel
            caminho_arquivo = self.salvar_excel(df, arquivo_saida)
            
            logger.info("=== PIPELINE CONCLU√çDO COM SUCESSO ===")
            logger.info(f"Arquivo gerado: {caminho_arquivo}")
            logger.info(f"Total de registros: {len(df)}")
            
            return caminho_arquivo
            
        except Exception as e:
            logger.error(f"Erro no pipeline: {e}")
            raise


async def main():
    """Fun√ß√£o principal para executar o scraper"""
    
    print("üöÄ TabNews Data Engineering Scraper")
    print("=" * 50)
    
    # Configura√ß√µes (voc√™ pode alterar aqui)
    PAGINAS = 10        # N√∫mero de p√°ginas para buscar
    POR_PAGINA = 30     # Itens por p√°gina
    ARQUIVO_SAIDA = None  # None para nome autom√°tico
    
    try:
        extractor = TabNewsDataExtractor()
        
        caminho_arquivo = await extractor.executar_pipeline_completo(
            paginas=PAGINAS,
            por_pagina=POR_PAGINA,
            arquivo_saida=ARQUIVO_SAIDA
        )
        
        if caminho_arquivo:
            print(f"\n‚úÖ Sucesso! Arquivo Excel criado: {caminho_arquivo}")
        else:
            print("\n‚ö†Ô∏è  Nenhuma publica√ß√£o relevante encontrada.")
            
    except Exception as e:
        print(f"\n‚ùå Erro na execu√ß√£o: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    # Execute o script
    exit_code = asyncio.run(main())
    exit(exit_code)
